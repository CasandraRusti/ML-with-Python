{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports for all practice\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mglearn\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# scaler = MinMaxScaler()\n",
    "# mlpscaler = StandardScaler()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "#a= time.time()\n",
    "#b= time.time()\n",
    "#print(\"Run time: {:.4f}\".format(b-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (1797, 64)\n",
      "Scaled Data shape: (1797, 64)\n",
      "per-feature minimum before scaling:\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "per-feature maximum before scaling:\n",
      " [ 0.  8. 16. 16. 16. 16. 16. 15.  2. 16. 16. 16. 16. 16. 16. 11.  2. 16.\n",
      " 16. 16. 16. 16. 16.  8.  1. 15. 16. 16. 16. 16. 15.  1.  0. 14. 16. 16.\n",
      " 16. 16. 14.  0.  4. 16. 16. 16. 16. 16. 16.  6.  1. 15. 16. 16. 16. 16.\n",
      " 16. 13.  0.  9. 16. 16. 16. 16. 16. 16.]\n",
      "per-feature minimum after scaling:\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "per-feature maximum after scaling:\n",
      " [0.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         0.91666667 1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.         1.         1.         1.\n",
      " 1.         1.         1.         0.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.125      0.9375     1.         1.         1.         1.\n",
      " 1.         1.         0.         1.         1.         1.\n",
      " 1.         1.         1.         1.        ]\n",
      "MLP Scaled Data shape: (1797, 64)\n",
      "MLP per-feature minimum before scaling:\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "MLP per-feature maximum before scaling:\n",
      " [ 0.  8. 16. 16. 16. 16. 16. 15.  2. 16. 16. 16. 16. 16. 16. 11.  2. 16.\n",
      " 16. 16. 16. 16. 16.  8.  1. 15. 16. 16. 16. 16. 15.  1.  0. 14. 16. 16.\n",
      " 16. 16. 14.  0.  4. 16. 16. 16. 16. 16. 16.  6.  1. 15. 16. 16. 16. 16.\n",
      " 16. 13.  0.  9. 16. 16. 16. 16. 16. 16.]\n",
      "MLP per-feature minimum after scaling:\n",
      " [ 0.         -0.33501649 -1.09493684 -2.78643746 -2.76424203 -1.02065685\n",
      " -0.40972392 -0.12502292 -0.05907756 -0.62400926 -1.91557297 -3.01259995\n",
      " -2.14988554 -1.35109507 -0.51499146 -0.13043338 -0.04462507 -0.72764628\n",
      " -1.74070185 -1.20543167 -1.14964846 -1.25998248 -0.54880546 -0.11422184\n",
      " -0.03337973 -0.78510524 -1.46862699 -1.49990136 -1.61406277 -1.28625035\n",
      " -0.62889588 -0.04723238  0.         -0.67237227 -1.21260656 -1.44763006\n",
      " -1.73666443 -1.48986148 -0.82269451  0.         -0.06134367 -0.5312841\n",
      " -1.05283456 -1.12245711 -1.22603292 -1.44653841 -0.79827225 -0.08874162\n",
      " -0.03543326 -0.40357499 -1.33033057 -1.82551805 -1.77645875 -1.45261152\n",
      " -0.75743581 -0.20978513 -0.02359646 -0.29908135 -1.08938309 -2.76417101\n",
      " -2.39411016 -1.14664746 -0.5056698  -0.19600752]\n",
      "MLP per-feature maximum after scaling:\n",
      " [ 0.00000000e+00  8.48585718e+00  2.27100180e+00  9.80342825e-01\n",
      "  9.68672669e-01  1.80378259e+00  4.40252370e+00  1.43384659e+01\n",
      "  2.11733965e+01  4.38339077e+00  1.03648545e+00  1.01110394e+00\n",
      "  1.19644995e+00  1.29297554e+00  3.94764700e+00  1.31596524e+01\n",
      "  3.20318776e+01  3.74749508e+00  1.07165259e+00  1.55269115e+00\n",
      "  1.44186017e+00  1.32249658e+00  4.36073240e+00  1.81308140e+01\n",
      "  2.99583044e+01  3.98337420e+00  1.11605583e+00  1.22058589e+00\n",
      "  9.87402154e-01  1.43904590e+00  3.44118539e+00  2.11719154e+01\n",
      "  0.00000000e+00  3.35130558e+00  1.31786694e+00  1.10556952e+00\n",
      "  9.60644107e-01  1.23632049e+00  3.13624745e+00  0.00000000e+00\n",
      "  2.74972996e+01  4.83606670e+00  1.39509519e+00  1.36217503e+00\n",
      "  1.33078862e+00  1.36346581e+00  2.89709463e+00  1.94380363e+01\n",
      "  4.86253388e+00  8.18912939e+00  1.50507822e+00  1.23639392e+00\n",
      "  1.24208282e+00  1.20101872e+00  2.49589470e+00  1.29998924e+01\n",
      " -2.35964589e-02  9.33646172e+00  2.04688832e+00  8.94245713e-01\n",
      "  8.49632138e-01  1.56568555e+00  3.40687545e+00  8.40797443e+00]\n"
     ]
    }
   ],
   "source": [
    "# Import and split the data\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(\"Data shape: {}\".format(digits.data.shape))\n",
    "\n",
    "## took out stratify=digits.target QUESTION: what does that piece do?\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    digits.data, digits.target, random_state=69)\n",
    "\n",
    "## SCALE & split scaled data for SVM\n",
    "X_scaled = MinMaxScaler().fit_transform(digits.data)\n",
    "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(\n",
    "    X_scaled, digits.target, random_state=69)\n",
    "print(\"Scaled Data shape: {}\".format(X_scaled.shape))\n",
    "\n",
    "print(\"per-feature minimum before scaling:\\n {}\".format(X_train.min(axis=0)))\n",
    "print(\"per-feature maximum before scaling:\\n {}\".format(X_train.max(axis=0)))\n",
    "print(\"per-feature minimum after scaling:\\n {}\".format(\n",
    "X_train_scaled.min(axis=0)))\n",
    "print(\"per-feature maximum after scaling:\\n {}\".format(\n",
    "X_train_scaled.max(axis=0)))\n",
    "\n",
    "## SCALE & split scaled data for MLP (Neural Networks)\n",
    "X_scaled2 = StandardScaler().fit_transform(digits.data)\n",
    "X_train_scaled2, X_test_scaled2, y_train, y_test = train_test_split(\n",
    "    X_scaled2, digits.target, random_state=69)\n",
    "print(\"MLP Scaled Data shape: {}\".format(X_scaled2.shape))\n",
    "\n",
    "print(\"MLP per-feature minimum before scaling:\\n {}\".format(X_train.min(axis=0)))\n",
    "print(\"MLP per-feature maximum before scaling:\\n {}\".format(X_train.max(axis=0)))\n",
    "print(\"MLP per-feature minimum after scaling:\\n {}\".format(\n",
    "X_train_scaled2.min(axis=0)))\n",
    "print(\"MLP per-feature maximum after scaling:\\n {}\".format(\n",
    "X_train_scaled2.max(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainng set score: 0.9955\n",
      "Test set score: 0.9622\n",
      "Run time: 0.2349\n"
     ]
    }
   ],
   "source": [
    "## ROW 3\n",
    "\n",
    "a= time.time()\n",
    "\n",
    "# DEFAULT parameters: Logistic Regression\n",
    "## added more iterations to get rid of warning\n",
    "logreg = LogisticRegression(solver='liblinear',max_iter=10000).fit(X_train, y_train)\n",
    "\n",
    "print(\"Trainng set score: {:.4f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.4f}\".format(logreg.score(X_test, y_test)))\n",
    "\n",
    "b= time.time()\n",
    "print(\"Run time: {:.4f}\".format(b-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainng set score: 1.0000\n",
      "Test set score: 0.9733\n",
      "Run time: 0.1119\n"
     ]
    }
   ],
   "source": [
    "## ROW 4\n",
    "\n",
    "a= time.time()\n",
    "\n",
    "# CUSTOM parameters: Logistic Regression\n",
    "## added more iterations to get rid of warning\n",
    "logreg_cust = LogisticRegression(C=10000000,max_iter=100000).fit(X_train, y_train)\n",
    "print(\"Trainng set score: {:.4f}\".format(logreg_cust.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.4f}\".format(logreg_cust.score(X_test, y_test)))\n",
    "\n",
    "b= time.time()\n",
    "print(\"Run time: {:.4f}\".format(b-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainng set score: 1.0000\n",
      "Test set score: 0.9756\n",
      "Run time: 0.4028\n"
     ]
    }
   ],
   "source": [
    "## ROW 5\n",
    "\n",
    "a= time.time()\n",
    "\n",
    "# DEFAULT parameters: Random Forest\n",
    "forest = RandomForestClassifier(random_state=69).fit(X_train, y_train)\n",
    "print(\"Trainng set score: {:.4f}\".format(forest.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.4f}\".format(forest.score(X_test, y_test)))\n",
    "\n",
    "b= time.time()\n",
    "print(\"Run time: {:.4f}\".format(b-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainng set score: 1.0000\n",
      "Test set score: 0.9756\n",
      "Run time: 4.3575\n"
     ]
    }
   ],
   "source": [
    "## ROW 6\n",
    "\n",
    "a= time.time()\n",
    "\n",
    "# CUSTOM parameters: Random Forest\n",
    "forest_cust = RandomForestClassifier(n_estimators=1000, random_state=69).fit(X_train, y_train)\n",
    "print(\"Trainng set score: {:.4f}\".format(forest_cust.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.4f}\".format(forest_cust.score(X_test, y_test)))\n",
    "\n",
    "b= time.time()\n",
    "print(\"Run time: {:.4f}\".format(b-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainng set score: 1.0000\n",
      "Test set score: 0.9644\n",
      "Run time: 8.2673\n"
     ]
    }
   ],
   "source": [
    "## ROW 7\n",
    "\n",
    "a= time.time()\n",
    "\n",
    "# DEFAULT parameters: Gradient Boosting\n",
    "gbrt = GradientBoostingClassifier(random_state=69).fit(X_train, y_train)\n",
    "print(\"Trainng set score: {:.4f}\".format(gbrt.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.4f}\".format(gbrt.score(X_test, y_test)))\n",
    "\n",
    "b= time.time()\n",
    "print(\"Run time: {:.4f}\".format(b-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainng set score: 1.0000\n",
      "Test set score: 0.9489\n",
      "Run time: 3.4716\n"
     ]
    }
   ],
   "source": [
    "## ROW 8 - learning rate\n",
    "\n",
    "a= time.time()\n",
    "\n",
    "# CUSTOM parameters: Gradient Boosting\n",
    "## parameters to play with are learning_rate OR max_depth\n",
    "gbrt_cust1 = GradientBoostingClassifier(learning_rate=0.5,random_state=69).fit(X_train, y_train)\n",
    "print(\"Trainng set score: {:.4f}\".format(gbrt_cust1.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.4f}\".format(gbrt_cust1.score(X_test, y_test)))\n",
    "\n",
    "b= time.time()\n",
    "print(\"Run time: {:.4f}\".format(b-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainng set score: 1.0000\n",
      "Test set score: 0.9644\n",
      "Run time: 6.9884\n"
     ]
    }
   ],
   "source": [
    "## ROW 8 - max depth\n",
    "\n",
    "a= time.time()\n",
    "\n",
    "# CUSTOM parameters: Gradient Boosting\n",
    "## parameters to play with are learning_rate OR max_depth\n",
    "gbrt_cust2 = GradientBoostingClassifier(max_depth=2,random_state=69).fit(X_train, y_train)\n",
    "print(\"Trainng set score: {:.4f}\".format(gbrt_cust2.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.4f}\".format(gbrt_cust2.score(X_test, y_test)))\n",
    "\n",
    "b= time.time()\n",
    "print(\"Run time: {:.4f}\".format(b-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainng set score: 0.9703\n",
      "Test set score: 0.9711\n",
      "Run time: 0.9134\n"
     ]
    }
   ],
   "source": [
    "## ROW 9\n",
    "\n",
    "a= time.time()\n",
    "\n",
    "# DEFAULT parameters: SVM\n",
    "svc = SVC(gamma='auto').fit(X_train_scaled, y_train)\n",
    "print(\"Trainng set score: {:.4f}\".format(svc.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.4f}\".format(svc.score(X_test_scaled, y_test)))\n",
    "\n",
    "b= time.time()\n",
    "print(\"Run time: {:.4f}\".format(b-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainng set score: 1.0000\n",
      "Test set score: 0.9911\n",
      "Run time: 0.1469\n"
     ]
    }
   ],
   "source": [
    "## ROW 10\n",
    "\n",
    "a= time.time()\n",
    "\n",
    "# CUSTOM parameters: SVM\n",
    "svc = SVC(C=1000).fit(X_train_scaled, y_train)\n",
    "print(\"Trainng set score: {:.4f}\".format(svc.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.4f}\".format(svc.score(X_test_scaled, y_test)))\n",
    "\n",
    "b= time.time()\n",
    "print(\"Run time: {:.4f}\".format(b-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainng set score: 1.0000\n",
      "Test set score: 0.9756\n",
      "Run time: 0.2448\n"
     ]
    }
   ],
   "source": [
    "## ROW 11\n",
    "\n",
    "a= time.time()\n",
    "\n",
    "# DEFAULT parameters: Neural Networks\n",
    "mlp = MLPClassifier(solver='lbfgs',max_iter=1000000000,random_state=69).fit(X_train_scaled2, y_train)\n",
    "print(\"Trainng set score: {:.4f}\".format(mlp.score(X_train_scaled2, y_train)))\n",
    "print(\"Test set score: {:.4f}\".format(mlp.score(X_test_scaled2, y_test)))\n",
    "\n",
    "b= time.time()\n",
    "print(\"Run time: {:.4f}\".format(b-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainng set score: 1.0000\n",
      "Test set score: 0.9756\n",
      "Run time: 2.3751\n"
     ]
    }
   ],
   "source": [
    "## ROW 12\n",
    "\n",
    "a= time.time()\n",
    "\n",
    "# CUSTOM parameters: Neural Networks\n",
    "## tried hidden_layer_sizes=[100, 100] and and 50,50\n",
    "mlp_cust = MLPClassifier(hidden_layer_sizes=[75, 75],random_state=69).fit(X_train_scaled2, y_train)\n",
    "print(\"Trainng set score: {:.4f}\".format(mlp_cust.score(X_train_scaled2, y_train)))\n",
    "print(\"Test set score: {:.4f}\".format(mlp_cust.score(X_test_scaled2, y_test)))\n",
    "\n",
    "b= time.time()\n",
    "print(\"Run time: {:.4f}\".format(b-a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
